{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e6c39-a496-462b-9483-ba36d30ba944",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "import gc, os\n",
    "gc.collect()\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fab8ac-b5b3-46a2-a090-0c17a16aaed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark iniciado SIN conector Snowflake\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CleanSpark_NoSnowflake\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark iniciado SIN conector Snowflake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634d8e2b-cbc2-410a-9378-257a1e1c56c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesi√≥n Spark y conexi√≥n Snowflake listas.\n"
     ]
    }
   ],
   "source": [
    "# === Reinicializaci√≥n de entorno y sesi√≥n Spark ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Deber03_Enriquecimiento\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SF_OPTIONS = {\n",
    "    \"sfURL\": f\"{os.getenv('SNOWFLAKE_ACCOUNT')}.snowflakecomputing.com\",\n",
    "    \"sfAccount\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"sfUser\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"sfPassword\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"sfDatabase\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"sfSchema\": os.getenv(\"SNOWFLAKE_SCHEMA_RAW\", \"RAW\"),\n",
    "}\n",
    "\n",
    "print(\"Sesi√≥n Spark y conexi√≥n Snowflake listas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f925be-2fc7-4fbb-80ea-6b8e81a48f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+\n",
      "|TABLE_NAME     |ROW_COUNT|\n",
      "+---------------+---------+\n",
      "|YELLOW_2019_01 |7696617  |\n",
      "|YELLOW_2019_02 |7049370  |\n",
      "|CONN_TEST_SPARK|1        |\n",
      "|GREEN_2019_02  |615594   |\n",
      "|GREEN_2019_01  |672105   |\n",
      "|INGEST_AUDIT   |4        |\n",
      "+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "df_check = (\n",
    "    spark.read.format(\"snowflake\")\n",
    "    .options(**SF_OPTIONS)\n",
    "    .option(\n",
    "        \"query\",\n",
    "        \"SELECT TABLE_NAME, ROW_COUNT FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'RAW'\"\n",
    "    )\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df_check.show(50, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be102b44-b74c-446b-b0f9-6cbeaa4e2912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo RAW.YELLOW_2019_01\n",
      "Leyendo RAW.YELLOW_2019_02\n",
      "Leyendo RAW.GREEN_2019_01\n",
      "Leyendo RAW.GREEN_2019_02\n",
      "Uni√≥n completa: 16,033,686 filas totales\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "YEARS   = [2019]         # Parametrizable 2015‚Äì2025\n",
    "MONTHS  = [1, 2]         # Parametrizable 1‚Äì12\n",
    "SERVICES= [\"yellow\",\"green\"]\n",
    "\n",
    "def read_raw(service, year, month):\n",
    "    tbl = f\"RAW.{service.upper()}_{year}_{month:02d}\"\n",
    "    print(\"Leyendo\", tbl)\n",
    "    return (spark.read.format(\"snowflake\")\n",
    "            .options(**SF_OPTIONS)\n",
    "            .option(\"dbtable\", tbl)\n",
    "            .load()\n",
    "            .withColumn(\"service_type\", F.lit(service))\n",
    "            .withColumn(\"source_year\",  F.lit(year))\n",
    "            .withColumn(\"source_month\", F.lit(month)))\n",
    "\n",
    "dfs = []\n",
    "for s in SERVICES:\n",
    "    for y in YEARS:\n",
    "        for m in MONTHS:\n",
    "            try:\n",
    "                dfs.append(read_raw(s, y, m))\n",
    "            except Exception as e:\n",
    "                print(\"Saltando\", s, y, m, e)\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    raise RuntimeError(\"No se carg√≥ ninguna tabla RAW. Verifica esquema o nombres.\")\n",
    "else:\n",
    "    raw_union = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        raw_union = raw_union.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "    print(f\"Uni√≥n completa: {raw_union.count():,} filas totales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b43a8f6-3cd4-4a28-9604-afd120e798de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando lookup de zonas...\n",
      "Descarga completada: /tmp/taxi_zone_lookup.csv\n",
      "Taxi Zones cargadas: 265 registros\n",
      "+-----------+-------------+--------------------+\n",
      "|location_id|      borough|                zone|\n",
      "+-----------+-------------+--------------------+\n",
      "|          1|          EWR|      Newark Airport|\n",
      "|          2|       Queens|         Jamaica Bay|\n",
      "|          3|        Bronx|Allerton/Pelham G...|\n",
      "|          4|    Manhattan|       Alphabet City|\n",
      "|          5|Staten Island|       Arden Heights|\n",
      "+-----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests, tempfile\n",
    "\n",
    "zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "tmp_path = os.path.join(tempfile.gettempdir(), \"taxi_zone_lookup.csv\")\n",
    "\n",
    "# Descargar el archivo a una ruta temporal\n",
    "print(\"Descargando lookup de zonas...\")\n",
    "try:\n",
    "    r = requests.get(zones_url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    open(tmp_path, \"wb\").write(r.content)\n",
    "    print(\"Descarga completada:\", tmp_path)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No se pudo descargar {zones_url}: {e}\")\n",
    "\n",
    "# Leerlo con Spark\n",
    "zones = (\n",
    "    spark.read.csv(tmp_path, header=True)\n",
    "    .select(\n",
    "        F.col(\"LocationID\").cast(\"int\").alias(\"location_id\"),\n",
    "        F.col(\"Borough\").alias(\"borough\"),\n",
    "        F.col(\"Zone\").alias(\"zone\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Taxi Zones cargadas:\", zones.count(), \"registros\")\n",
    "zones.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35054f3-ca6c-499b-8b5a-cbc1f27a8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset enriquecido con zonas NYC Taxi\n",
      "Filas totales: 16033686\n",
      "+------------+-------------------+----------+--------------------+----------+--------------------+\n",
      "|service_type|    pickup_datetime|pu_borough|             pu_zone|do_borough|             do_zone|\n",
      "+------------+-------------------+----------+--------------------+----------+--------------------+\n",
      "|      yellow|2019-01-01 11:19:45| Manhattan|Times Sq/Theatre ...| Manhattan|            Gramercy|\n",
      "|      yellow|2019-01-01 11:41:24| Manhattan|            Union Sq| Manhattan|        East Chelsea|\n",
      "|      yellow|2019-01-01 11:39:40| Manhattan|Upper East Side S...| Manhattan|Upper East Side N...|\n",
      "|      yellow|2019-01-01 11:25:47| Manhattan| Morningside Heights| Manhattan| Lincoln Square East|\n",
      "|      yellow|2019-01-01 11:46:22| Manhattan|Upper East Side S...| Manhattan|        Midtown East|\n",
      "|      yellow|2019-01-01 11:18:00| Manhattan|Financial Distric...|  Brooklyn|Williamsburg (Sou...|\n",
      "|      yellow|2019-01-01 11:39:32|  Brooklyn|Williamsburg (Sou...|  Brooklyn|Williamsburg (Nor...|\n",
      "|      yellow|2019-01-01 11:49:59| Manhattan| Lincoln Square East| Manhattan|Upper West Side S...|\n",
      "|      yellow|2019-01-01 11:19:37| Manhattan|        Clinton East| Manhattan|      Midtown Center|\n",
      "|      yellow|2019-01-01 11:38:48| Manhattan|    Garment District| Manhattan|   Battery Park City|\n",
      "|      yellow|2019-01-01 11:40:37| Manhattan|  World Trade Center| Manhattan|     Lenox Hill East|\n",
      "|      yellow|2019-01-01 11:55:30| Manhattan|Sutton Place/Turt...| Manhattan|Upper East Side S...|\n",
      "|      yellow|2019-01-01 11:29:46|    Queens|Briarwood/Jamaica...|    Queens|        Forest Hills|\n",
      "|      yellow|2019-01-01 11:03:07| Manhattan|            Gramercy| Manhattan|       Midtown South|\n",
      "|      yellow|2019-01-01 11:22:53| Manhattan|Times Sq/Theatre ...| Manhattan|Upper West Side N...|\n",
      "|      yellow|2019-01-01 11:43:53| Manhattan|Upper West Side N...| Manhattan|Greenwich Village...|\n",
      "|      yellow|2019-01-01 11:03:43| Manhattan|Greenwich Village...| Manhattan|Greenwich Village...|\n",
      "|      yellow|2019-01-01 11:37:39|   Unknown|                 N/A| Manhattan| Lincoln Square West|\n",
      "|      yellow|2019-01-01 11:40:29|  Brooklyn|  DUMBO/Vinegar Hill|  Brooklyn|      Bushwick North|\n",
      "|      yellow|2019-01-01 11:33:56| Manhattan|Upper East Side S...| Manhattan| Lincoln Square East|\n",
      "+------------+-------------------+----------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enriquecer con zonas pickup/dropoff\n",
    "raw_enriched = (\n",
    "    raw_union\n",
    "    # Join con zonas de origen (pickup)\n",
    "    .join(\n",
    "        zones.withColumnRenamed(\"location_id\", \"PULocationID\"),\n",
    "        on=\"PULocationID\", how=\"left\"\n",
    "    )\n",
    "    .withColumnRenamed(\"borough\", \"pu_borough\")\n",
    "    .withColumnRenamed(\"zone\", \"pu_zone\")\n",
    "    # Join con zonas de destino (dropoff)\n",
    "    .join(\n",
    "        zones.withColumnRenamed(\"location_id\", \"DOLocationID\"),\n",
    "        on=\"DOLocationID\", how=\"left\"\n",
    "    )\n",
    "    .withColumnRenamed(\"borough\", \"do_borough\")\n",
    "    .withColumnRenamed(\"zone\", \"do_zone\")\n",
    ")\n",
    "\n",
    "print(\"Dataset enriquecido con zonas NYC Taxi\")\n",
    "print(\"Filas totales:\", raw_enriched.count())\n",
    "raw_enriched.select(\"service_type\", \"pickup_datetime\", \"pu_borough\", \"pu_zone\", \"do_borough\", \"do_zone\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23029e50-b558-43c3-a937-0294ae0e27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat√°logos normalizados (payment_type, ratecode, vendor)\n",
      "+--------+----------------------------+----------+--------------+------------+-----------------+\n",
      "|VendorID|vendor_name                 |RatecodeID|rate_code_desc|payment_type|payment_type_desc|\n",
      "+--------+----------------------------+----------+--------------+------------+-----------------+\n",
      "|2       |VeriFone Inc                |1.0       |Standard rate |2.0         |Cash             |\n",
      "|2       |VeriFone Inc                |1.0       |Standard rate |2.0         |Cash             |\n",
      "|1       |Creative Mobile Technologies|1.0       |Standard rate |1.0         |Credit card      |\n",
      "|2       |VeriFone Inc                |1.0       |Standard rate |1.0         |Credit card      |\n",
      "|2       |VeriFone Inc                |1.0       |Standard rate |1.0         |Credit card      |\n",
      "+--------+----------------------------+----------+--------------+------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Mapas de referencia\n",
    "payment_map = F.create_map([F.lit(x) for x in\n",
    "   [1,\"Credit card\", 2,\"Cash\", 3,\"No charge\", 4,\"Dispute\", 5,\"Unknown\", 6,\"Voided trip\"]])\n",
    "\n",
    "rate_map = F.create_map([F.lit(x) for x in\n",
    "   [1,\"Standard rate\", 2,\"JFK\", 3,\"Newark\", 4,\"Nassau/Westchester\", 5,\"Negotiated fare\", 6,\"Group ride\"]])\n",
    "\n",
    "vendor_map = F.create_map([F.lit(x) for x in\n",
    "   [1,\"Creative Mobile Technologies\", 2,\"VeriFone Inc\"]])\n",
    "\n",
    "# Aplicar descripciones\n",
    "raw_enriched = (\n",
    "    raw_enriched\n",
    "    .withColumn(\"payment_type_desc\", payment_map[F.col(\"payment_type\").cast(\"int\")])\n",
    "    .withColumn(\"rate_code_desc\", rate_map[F.col(\"RatecodeID\").cast(\"int\")])\n",
    "    .withColumn(\"vendor_name\", vendor_map[F.col(\"VendorID\").cast(\"int\")])\n",
    ")\n",
    "\n",
    "print(\"Cat√°logos normalizados (payment_type, ratecode, vendor)\")\n",
    "raw_enriched.select(\n",
    "    \"VendorID\", \"vendor_name\",\n",
    "    \"RatecodeID\", \"rate_code_desc\",\n",
    "    \"payment_type\", \"payment_type_desc\"\n",
    ").show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb175822-f408-44c7-a5b4-9dfc68e8e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema ANALYTICS verificado/creado.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=os.getenv(\"SNOWFLAKE_DATABASE\")\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE SCHEMA IF NOT EXISTS ANALYTICS;\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Esquema ANALYTICS verificado/creado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec82f56-2e3f-4f0f-a9da-8268ce58c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla ANALYTICS.TRIPS_ENRICHED recreada con columnas descriptivas.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    schema=\"ANALYTICS\"\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Ahora incluimos las columnas descriptivas\n",
    "cur.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE TRIPS_ENRICHED (\n",
    "    vendorid INTEGER,\n",
    "    vendor_name STRING,\n",
    "    pickup_datetime TIMESTAMP_NTZ,\n",
    "    dropoff_datetime TIMESTAMP_NTZ,\n",
    "    passenger_count FLOAT,\n",
    "    trip_distance FLOAT,\n",
    "    ratecodeid FLOAT,\n",
    "    rate_code_desc STRING,\n",
    "    store_and_fwd_flag STRING,\n",
    "    pulocationid INTEGER,\n",
    "    dolocationid INTEGER,\n",
    "    payment_type FLOAT,\n",
    "    payment_type_desc STRING,\n",
    "    fare_amount FLOAT,\n",
    "    extra FLOAT,\n",
    "    mta_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    improvement_surcharge FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    congestion_surcharge FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    service_type STRING,\n",
    "    source_year INTEGER,\n",
    "    source_month INTEGER,\n",
    "    run_id STRING,\n",
    "    ingested_at_utc TIMESTAMP_NTZ,\n",
    "    source_path STRING,\n",
    "    pu_borough STRING,\n",
    "    pu_zone STRING,\n",
    "    do_borough STRING,\n",
    "    do_zone STRING\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabla ANALYTICS.TRIPS_ENRICHED recreada con columnas descriptivas.\")\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f4913f8-6445-4230-bf90-ab7e22062115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Eliminando Parquets temporales antiguos...\n",
      " Borrado yellow_tripdata_2019-01.parquet\n",
      " Borrado yellow_tripdata_2019-02.parquet\n",
      " Borrado green_tripdata_2019-01.parquet\n",
      " Borrado green_tripdata_2019-02.parquet\n",
      "  Carpeta trips_enriched.parquet eliminada\n",
      "Limpieza completa del directorio temporal /tmp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "tmp_dir = \"/tmp\"\n",
    "delete_targets = [\n",
    "    \"yellow_tripdata_2019-01.parquet\",\n",
    "    \"yellow_tripdata_2019-02.parquet\",\n",
    "    \"green_tripdata_2019-01.parquet\",\n",
    "    \"green_tripdata_2019-02.parquet\",\n",
    "]\n",
    "\n",
    "print(\"üßπ Eliminando Parquets temporales antiguos...\")\n",
    "for f in delete_targets:\n",
    "    path = os.path.join(tmp_dir, f)\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(f\" Borrado {f}\")\n",
    "    else:\n",
    "        print(f\" No encontrado {f}\")\n",
    "\n",
    "# (opcional) limpia tambi√©n el directorio de trips_enriched si est√° vac√≠o\n",
    "enriched_dir = os.path.join(tmp_dir, \"trips_enriched.parquet\")\n",
    "if os.path.exists(enriched_dir):\n",
    "    shutil.rmtree(enriched_dir)\n",
    "    print(\"  Carpeta trips_enriched.parquet eliminada\")\n",
    "\n",
    "print(\"Limpieza completa del directorio temporal /tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6bf19d4-fa70-499b-a941-15eda8cf743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Carpeta temporal actual: /tmp\n",
      " No hay archivos Parquet residuales\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "tmp_dir = tempfile.gettempdir()\n",
    "print(\" Carpeta temporal actual:\", tmp_dir)\n",
    "!ls -lh $tmp_dir | grep parquet || echo \" No hay archivos Parquet residuales\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f27c38-7128-48f3-b6f9-b7e0a67b4cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452M\t/tmp\n"
     ]
    }
   ],
   "source": [
    "!du -sh /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4edc63-83da-44e1-8c1f-4e037983ff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportando a /tmp/trips_enriched_final\n",
      "Exportaci√≥n local completada\n"
     ]
    }
   ],
   "source": [
    "import os, tempfile, shutil\n",
    "\n",
    "tmp_dir = os.path.join(tempfile.gettempdir(), \"trips_enriched_final\")\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "print(f\"Exportando a {tmp_dir}\")\n",
    "raw_enriched.coalesce(1).write.mode(\"overwrite\").parquet(tmp_dir)\n",
    "print(\"Exportaci√≥n local completada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9aed598-6738-47e7-8cc6-f62fc78028ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido de: /tmp/trips_enriched_final\n",
      "['.part-00000-58bada99-d77f-4526-82d1-4a64d1a1a69b-c000.snappy.parquet.crc', 'part-00000-58bada99-d77f-4526-82d1-4a64d1a1a69b-c000.snappy.parquet', '._SUCCESS.crc', '_SUCCESS']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tmp_dir = \"/tmp/trips_enriched_final\"\n",
    "print(\"Contenido de:\", tmp_dir)\n",
    "\n",
    "if os.path.exists(tmp_dir):\n",
    "    print(os.listdir(tmp_dir))\n",
    "else:\n",
    "    print(\"La carpeta no existe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f657bf-c6a7-4dc1-a400-db3085e64c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo /tmp/trips_enriched_final/part-00000-58bada99-d77f-4526-82d1-4a64d1a1a69b-c000.snappy.parquet ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1Ô∏èCargar variables de entorno\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "#  Buscar parquet exportado\n",
    "tmp_dir = \"/tmp/trips_enriched_final\"\n",
    "parquet_file = glob.glob(os.path.join(tmp_dir, \"*.parquet\"))[0]\n",
    "\n",
    "print(f\"Leyendo {parquet_file} ...\")\n",
    "pdf = pd.read_parquet(parquet_file)\n",
    "print(f\"DataFrame le√≠do con {len(pdf):,} filas y {len(pdf.columns)} columnas\")\n",
    "\n",
    "# Conexi√≥n Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    schema=\"ANALYTICS\"\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Subir en chunks (bloques peque√±os)\n",
    "chunk_size = 50000  # ajusta si quieres menos/m√°s filas por lote\n",
    "total_rows = len(pdf)\n",
    "for i in range(0, total_rows, chunk_size):\n",
    "    chunk = pdf.iloc[i:i+chunk_size]\n",
    "    success, nchunks, nrows, _ = cur.write_pandas(\n",
    "        chunk,\n",
    "        table_name=\"TRIPS_ENRICHED\",\n",
    "        overwrite=(i == 0)  # solo el primer chunk reemplaza; los dem√°s hacen append\n",
    "    )\n",
    "    print(f\"Chunk {i//chunk_size + 1} ‚Üí {nrows} filas insertadas\")\n",
    "\n",
    "print(f\"Carga completa ‚Üí {total_rows:,} filas cargadas en ANALYTICS.TRIPS_ENRICHED\")\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872a40ee-a4b1-4a3b-9184-39be779c4379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subiendo: /tmp/trips_enriched_final/part-00000-58bada99-d77f-4526-82d1-4a64d1a1a69b-c000.snappy.parquet\n",
      "Carga completada en Snowflake.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector, os\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    schema=\"ANALYTICS\"\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Stage temporal\n",
    "cur.execute(\"CREATE OR REPLACE STAGE stage_trips_enriched;\")\n",
    "\n",
    "local_dir = \"/tmp/trips_enriched_final\"\n",
    "for file in os.listdir(local_dir):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        path = os.path.join(local_dir, file)\n",
    "        print(\"Subiendo:\", path)\n",
    "        cur.execute(f\"PUT file://{path} @stage_trips_enriched auto_compress=true\")\n",
    "\n",
    "# Cargar los datos con mapeo autom√°tico de columnas\n",
    "cur.execute(\"\"\"\n",
    "    COPY INTO TRIPS_ENRICHED\n",
    "    FROM @stage_trips_enriched\n",
    "    FILE_FORMAT=(TYPE=PARQUET)\n",
    "    MATCH_BY_COLUMN_NAME=CASE_INSENSITIVE\n",
    "    ON_ERROR=CONTINUE;\n",
    "\"\"\")\n",
    "\n",
    "print(\"Carga completada en Snowflake.\")\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1536e391-f124-47e8-883b-5a78c87171af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga verificada en ANALYTICS.TRIPS_ENRICHED (16033686 filas) - 2025-10-18 23:30:22.835409\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Verificaci√≥n final de carga ANALYTICS.TRIPS_ENRICHED \n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "# Crear sesi√≥n Spark (si no est√° activa)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"verificacion_analytics\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Configuraci√≥n de conexi√≥n Snowflake\n",
    "SF_OPTIONS_LOAD = {\n",
    "    \"sfURL\": f\"{os.getenv('SNOWFLAKE_ACCOUNT')}.snowflakecomputing.com\",  # corregido\n",
    "    \"sfDatabase\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"sfUser\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"sfPassword\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"sfSchema\": \"ANALYTICS\"\n",
    "}\n",
    "\n",
    "# Verificar conteo de registros en la tabla final\n",
    "count_snowflake = (\n",
    "    spark.read.format(\"snowflake\")\n",
    "    .options(**SF_OPTIONS_LOAD)\n",
    "    .option(\"dbtable\", \"TRIPS_ENRICHED\")\n",
    "    .load()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(f\"Carga verificada en ANALYTICS.TRIPS_ENRICHED ({count_snowflake} filas) - {datetime.now()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

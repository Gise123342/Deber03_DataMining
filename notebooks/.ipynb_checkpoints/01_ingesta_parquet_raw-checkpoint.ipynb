{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0747ac4-61af-46de-b78d-b132b3190917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: dev_0001\n",
      "YEARS: [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
      "MONTHS: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "SERVICES: ['yellow', 'green']\n",
      "PARQUET_BASE_URL: https://d37ci6vzurychx.cloudfront.net/trip-data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def expand_range_csv(spec):\n",
    "    if \"-\" in spec and \",\" not in spec:\n",
    "        a,b = spec.split(\"-\")\n",
    "        return list(range(int(a), int(b)+1))\n",
    "    return [int(x.strip()) for x in spec.split(\",\") if x.strip()]\n",
    "\n",
    "YEARS = expand_range_csv(os.getenv(\"YEARS\", \"2015\"))\n",
    "MONTHS = expand_range_csv(os.getenv(\"MONTHS\", \"1-12\"))\n",
    "SERVICES = [s.strip().lower() for s in os.getenv(\"SERVICES\",\"yellow,green\").split(\",\")]\n",
    "\n",
    "RUN_ID = os.getenv(\"RUN_ID\", f\"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\")\n",
    "PARQUET_BASE_URL = os.getenv(\"PARQUET_BASE_URL\")\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"YEARS:\", YEARS)\n",
    "print(\"MONTHS:\", MONTHS)\n",
    "print(\"SERVICES:\", SERVICES)\n",
    "print(\"PARQUET_BASE_URL:\", PARQUET_BASE_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56468fb6-a89e-49b8-85cf-63951ed64098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0.0.0.0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Deber03_Ingesta</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbf53f5c7d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Deber03_Ingesta\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f207b361-0236-4c7f-aa6f-788f3de3dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Snowflake en: ALNWMMV-NJ56428.snowflakecomputing.com\n"
     ]
    }
   ],
   "source": [
    "SF_OPTIONS = {\n",
    "    \"sfURL\": f\"{os.getenv('SNOWFLAKE_ACCOUNT')}.snowflakecomputing.com\",\n",
    "    \"sfAccount\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"sfUser\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"sfPassword\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"sfDatabase\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"sfSchema\": os.getenv(\"SNOWFLAKE_SCHEMA\", \"RAW\"),\n",
    "}\n",
    "SCHEMA_RAW = os.getenv(\"SNOWFLAKE_SCHEMA_RAW\", \"RAW\")\n",
    "print(\"Conectando a Snowflake en:\", SF_OPTIONS[\"sfURL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f87a6608-185e-4752-bfc5-3a7b36c157de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --- Snowflake ---\n",
      "SNOWFLAKE_ACCOUNT=ALNWMMV-NJ56428\n",
      "SNOWFLAKE_USER=USERSNOW\n",
      "SNOWFLAKE_PASSWORD=Password123456\n",
      "SNOWFLAKE_DATABASE=NYCTAXI_P3\n",
      "SNOWFLAKE_SCHEMA_RAW=RAW\n",
      "SNOWFLAKE_SCHEMA_ANALYTICS=ANALYTICS\n",
      "SNOWFLAKE_WAREHOUSE=COMPUTE_WH\n",
      "SNOWFLAKE_ROLE=ACCOUNTADMIN\n",
      "SNOWFLAKE_PRIVATE_KEY_PATH=null\n",
      "SNOWFLAKE_PRIVATE_KEY_PASSPHRASE=null\n",
      "SNOWFLAKE_TIMEOUT=null\n",
      "\n",
      "# --- Parámetros de procesamiento ---\n",
      "YEARS=2015-2025\n",
      "MONTHS=1-12\n",
      "SERVICES=yellow,green\n",
      "RUN_ID=dev_0001\n",
      "PARQUET_BASE_URL=https://d37ci6vzurychx.cloudfront.net/trip-data\n",
      "\n",
      "# --- Jupyter / Spark ---\n",
      "JUPYTER_TOKEN=class123\n",
      "JUPYTER_PORT=8888\n",
      "SPARK_UI_PORT=4040\n"
     ]
    }
   ],
   "source": [
    "!cat .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc596e62-6262-42a8-9a9f-447d0e259f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión OK: ('AWS_SA_EAST_1', '9.32.1')\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector, os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    database=os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    schema=os.getenv(\"SNOWFLAKE_SCHEMA_RAW\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT current_region(), current_version()\")\n",
    "print(\"Conexión OK:\", cur.fetchone())\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05dfeded-0aa9-4800-a158-e7ec37d03637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probando conexión Spark–Snowflake...\n",
      "Spark conectado correctamente a Snowflake RAW\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables desde .env\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Deber03_Ingesta_RAW\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "SF_OPTIONS = {\n",
    "    \"sfURL\": f\"{os.getenv('SNOWFLAKE_ACCOUNT')}.snowflakecomputing.com\",\n",
    "    \"sfAccount\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"sfUser\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"sfPassword\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"sfDatabase\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"sfSchema\": os.getenv(\"SNOWFLAKE_SCHEMA_RAW\", \"RAW\"),\n",
    "}\n",
    "\n",
    "print(\"Probando conexión Spark–Snowflake...\")\n",
    "test_df = spark.createDataFrame([(1, \"ok\")], [\"id\", \"msg\"])\n",
    "(\n",
    "    test_df.write\n",
    "    .format(\"snowflake\")\n",
    "    .options(**SF_OPTIONS)\n",
    "    .option(\"dbtable\", \"RAW.CONN_TEST_SPARK\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save()\n",
    ")\n",
    "print(\"Spark conectado correctamente a Snowflake RAW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8546a324-f4d0-4bd0-977e-9668c4382534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, tempfile\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "BASE_URL = os.getenv(\"PARQUET_BASE_URL\", \"https://d37ci6vzurychx.cloudfront.net/trip-data\")\n",
    "SCHEMA_RAW = os.getenv(\"SNOWFLAKE_SCHEMA_RAW\", \"RAW\")\n",
    "\n",
    "SERVICES = [\"yellow\", \"green\"]\n",
    "YEARS = [2019]   # luego puedes ampliar a 2015–2025\n",
    "MONTHS = [1, 2]  # luego ampliar a 1–12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49913ba2-a4a9-4b16-bda9-f87b8afb764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(service, year, month):\n",
    "    \"\"\"Descarga un Parquet, lo lee con Spark, estandariza columnas y agrega metadatos.\"\"\"\n",
    "    file_name = f\"{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    url = f\"{BASE_URL}/{file_name}\"\n",
    "    print(f\"Descargando {url}\")\n",
    "\n",
    "    tmp_path = os.path.join(tempfile.gettempdir(), file_name)\n",
    "    try:\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        open(tmp_path, \"wb\").write(r.content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No se pudo descargar {url}: {e}\")\n",
    "\n",
    "    # Leer con Spark\n",
    "    df = spark.read.parquet(tmp_path)\n",
    "\n",
    "    # Estandarizar fechas\n",
    "    if service == \"yellow\":\n",
    "        df = (\n",
    "            df\n",
    "            .withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "            .withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "        )\n",
    "    else:\n",
    "        df = (\n",
    "            df\n",
    "            .withColumnRenamed(\"lpep_pickup_datetime\", \"pickup_datetime\")\n",
    "            .withColumnRenamed(\"lpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "        )\n",
    "\n",
    "    # Agregar metadatos\n",
    "    run_id = f\"p3_run_{datetime.utcnow():%Y%m%d_%H%M%S}\"\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"service_type\", F.lit(service))\n",
    "        .withColumn(\"source_year\", F.lit(year))\n",
    "        .withColumn(\"source_month\", F.lit(month))\n",
    "        .withColumn(\"run_id\", F.lit(run_id))\n",
    "        .withColumn(\"ingested_at_utc\", F.lit(datetime.utcnow().isoformat()))\n",
    "        .withColumn(\"source_path\", F.lit(url))\n",
    "    )\n",
    "\n",
    "    print(f\"{service}_{year}-{month:02d} leído correctamente ({df.count():,} filas)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f766b02-1676-42fc-bd29-0351e71b0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_raw(df, service, year, month):\n",
    "    \"\"\"Escribe DataFrame en Snowflake RAW.{SERVICE}_{YEAR}_{MONTH}.\"\"\"\n",
    "    table_name = f\"{SCHEMA_RAW}.{service}_{year}_{month:02d}\"\n",
    "    options = dict(SF_OPTIONS)\n",
    "    options[\"dbtable\"] = table_name\n",
    "\n",
    "    # Convertir timestamps a string\n",
    "    for c in [\"pickup_datetime\", \"dropoff_datetime\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.date_format(F.col(c), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "    total_rows = df.count()\n",
    "    print(f\"Guardando en {table_name} ({total_rows:,} filas)...\")\n",
    "\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**options)\n",
    "        .mode(\"overwrite\")  # idempotente por mes/servicio\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "    print(f\"Escrito correctamente → {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "546c3c93-2f56-4dda-a886-efd3e662d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def log_audit(service, year, month, rows_ingested, run_id):\n",
    "    \"\"\"Registra metadatos de cada carga en la tabla RAW.INGEST_AUDIT.\"\"\"\n",
    "    audit_df = spark.createDataFrame([\n",
    "        Row(\n",
    "            service_type=service,\n",
    "            source_year=year,\n",
    "            source_month=month,\n",
    "            run_id=run_id,\n",
    "            rows_ingested=rows_ingested,\n",
    "            logged_at_utc=datetime.utcnow().isoformat()\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    (\n",
    "        audit_df.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**SF_OPTIONS)\n",
    "        .option(\"dbtable\", f\"{SCHEMA_RAW}.INGEST_AUDIT\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "    print(f\"Registro auditoría → RAW.INGEST_AUDIT ({service} {year}-{month:02d}: {rows_ingested:,} filas)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af734b55-6510-4d4b-b212-1135947e1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet\n",
      "yellow_2019-01 leído correctamente (7,696,617 filas)\n",
      "Guardando en RAW.yellow_2019_01 (7,696,617 filas)...\n",
      "Escrito correctamente → RAW.yellow_2019_01\n",
      "Registro auditoría → RAW.INGEST_AUDIT (yellow 2019-01: 7,696,617 filas)\n",
      "Descargando https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-02.parquet\n",
      "yellow_2019-02 leído correctamente (7,049,370 filas)\n",
      "Guardando en RAW.yellow_2019_02 (7,049,370 filas)...\n",
      "Escrito correctamente → RAW.yellow_2019_02\n",
      "Registro auditoría → RAW.INGEST_AUDIT (yellow 2019-02: 7,049,370 filas)\n",
      "Descargando https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-01.parquet\n",
      "green_2019-01 leído correctamente (672,105 filas)\n",
      "Guardando en RAW.green_2019_01 (672,105 filas)...\n",
      "Escrito correctamente → RAW.green_2019_01\n",
      "Registro auditoría → RAW.INGEST_AUDIT (green 2019-01: 672,105 filas)\n",
      "Descargando https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2019-02.parquet\n",
      "green_2019-02 leído correctamente (615,594 filas)\n",
      "Guardando en RAW.green_2019_02 (615,594 filas)...\n",
      "Escrito correctamente → RAW.green_2019_02\n",
      "Registro auditoría → RAW.INGEST_AUDIT (green 2019-02: 615,594 filas)\n"
     ]
    }
   ],
   "source": [
    "for s in SERVICES:\n",
    "    for y in YEARS:\n",
    "        for m in MONTHS:\n",
    "            try:\n",
    "                df = read_parquet(s, y, m)\n",
    "                write_to_raw(df, s, y, m)\n",
    "                rows = df.count()\n",
    "                run_id = df.select(\"run_id\").first()[\"run_id\"]\n",
    "                log_audit(s, y, m, rows, run_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error con {s} {y}-{m:02d}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
